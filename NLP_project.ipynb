{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stopwords?\n",
    "# stemmer or lemmatizer?\n",
    "# count instead of tfidf?\n",
    "# svd or highest feature frequency?\n",
    "# removed all punctuation here\n",
    "# 加入multinomialNB\n",
    "# 维持pos:neg的比例\n",
    "# pos tagging,换tagger\n",
    "# scp -P 49233 kaijiang@master1.internal.datalaus.net:~/output_data.csv .\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def read_json_self(path):\n",
    "    g = open(path,'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "        \n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in read_json_self(path):\n",
    "        df[i] = d\n",
    "        i+=1\n",
    "    return pd.DataFrame.from_dict(df,orient='index')\n",
    "\n",
    "kindle_data = getDF(\"reviews_Kindle_Store.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "    return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "kindle_data = getDF('reviews_Kindle_Store.json.gz')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = kindle_data.overall\n",
    "kindle_data.overall = kindle_data.overall.apply(lambda x : 'pos' if x > 3 else 'neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    2582347\n",
       "neg     623120\n",
       "Name: overall, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kindle_data.overall.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splitPosNeg(data):\n",
    "    neg = kindle_data.loc[data.overall=='neg']\n",
    "    pos = kindle_data.loc[data.overall=='pos']\n",
    "    return [pos,neg]\n",
    "\n",
    "[pos,neg] = splitPosNeg(kindle_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preprocessing steps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#stemmer = PorterStemmer()\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "stop = stopwords.words('english')\n",
    "translation = string.maketrans(string.punctuation,' '*len(string.punctuation))\n",
    "\n",
    "def preprocessing(line):\n",
    "    tokens=[]\n",
    "    line = line.translate(translation)\n",
    "    line = nltk.word_tokenize(line.lower())\n",
    "    for t in line:\n",
    "        if(t not in stop):\n",
    "            #stemmed = stemmer.stem(t)\n",
    "            stemmed = lemmatizer.lemmatize(t)\n",
    "            tokens.append(stemmed)\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_sample = pos.sample(frac=0.02)\n",
    "neg_sample = neg.sample(frac=0.004)\n",
    "\n",
    "# here i sample pos, so data more balanced and data size reduced\n",
    "# so we get 5*6000:6000 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_data = []\n",
    "neg_data = []\n",
    "#for p in pos.reviewText:\n",
    "for p in pos_sample['reviewText']:\n",
    "    pos_data.append(preprocessing(p))\n",
    "    \n",
    "#for p in neg.reviewText:\n",
    "for n in neg_sample['reviewText']:\n",
    "    neg_data.append(preprocessing(n))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pos_data + neg_data\n",
    "# remember this is sampled\n",
    "labels = np.concatenate((pos_sample['overall'].values,neg_sample['overall'].values))\n",
    "# labels = np.concatenate((pos['overall'].values,neg['overall'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_data = pd.DataFrame({'reviews': data,\n",
    "     'label': labels\n",
    "    })\n",
    "\n",
    "output_data.to_csv('biggest_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[Data_train,Data_test,Train_labels,Test_labels] = train_test_split(data,labels , test_size=0.2,stratify=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55202\n"
     ]
    }
   ],
   "source": [
    "## tokenizing the training data to find frequency of words\n",
    "t = []\n",
    "for line in Data_train:\n",
    "    l = nltk.word_tokenize(line)\n",
    "    for w in l:\n",
    "        t.append(w)\n",
    "        \n",
    "word_features = nltk.FreqDist(t)\n",
    "print (len(word_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# can consider PCA(which not apply to parse matrix, so use truncated svd) and feature selection based frequency here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor unigram\\n        Model  TF-IDF Accuracy\\n0  BernoulliNB         0.745175\\n1   Perceptron         0.816667\\n2          SVC         0.825877\\n3     Logistic         0.826754\\nfor unigram & bigram\\n        Model  TF-IDF Accuracy\\n0  BernoulliNB         0.752632\\n1   Perceptron         0.832456\\n2          SVC         0.835088\\n3     Logistic         0.834211\\n\\xe9\\x87\\x87\\xe7\\x94\\xa8bigram,\\xe8\\xbf\\x99\\xe4\\xb8\\xaa\\xe6\\x95\\xb0\\xe6\\x8d\\xae\\xe6\\x97\\xb6SVD=200\\nfor unigram & bigram & trigram\\n         Model  TF-IDF Accuracy\\n0  BernoulliNB         0.752193\\n1   Perceptron         0.709211\\n2          SVC         0.844298\\n3     Logistic         0.841667\\n'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for unigram\n",
    "        Model  TF-IDF Accuracy\n",
    "0  BernoulliNB         0.745175\n",
    "1   Perceptron         0.816667\n",
    "2          SVC         0.825877\n",
    "3     Logistic         0.826754\n",
    "for unigram & bigram\n",
    "        Model  TF-IDF Accuracy\n",
    "0  BernoulliNB         0.752632\n",
    "1   Perceptron         0.832456\n",
    "2          SVC         0.835088\n",
    "3     Logistic         0.834211\n",
    "采用bigram,这个数据时SVD=200\n",
    "for unigram & bigram & trigram\n",
    "         Model  TF-IDF Accuracy\n",
    "0  BernoulliNB         0.752193\n",
    "1   Perceptron         0.709211\n",
    "2          SVC         0.844298\n",
    "3     Logistic         0.841667\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vec_all = CountVectorizer()# unigram\n",
    "vec_all = CountVectorizer(ngram_range=(1,2))\n",
    "# unigram & bigram\n",
    "# vec_all = CountVectorizer(ngram_range=(1,3))\n",
    "# unigram & bigram & trigram\n",
    "ctr_features_all = vec_all.fit_transform(Data_train)\n",
    "\n",
    "tf_vec_all = TfidfTransformer()\n",
    "tr_features_all = tf_vec_all.fit_transform(ctr_features_all) # train data\n",
    "\n",
    "cte_features_all = vec_all.transform(Data_test)\n",
    "te_features_all = tf_vec_all.transform(cte_features_all) # test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43311, 1005422)\n"
     ]
    }
   ],
   "source": [
    "print tr_features_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 不用SVD效果更差？\n",
    "svd = TruncatedSVD(n_components=200)\n",
    "tr_features_truncated = svd.fit_transform(tr_features_all)\n",
    "\n",
    "te_features_truncated = svd.transform(te_features_all)\n",
    "\n",
    "#svd = TruncatedSVD(n_components=200)\n",
    "#ctr_features_truncated = svd.fit_transform(ctr_features_all)\n",
    "#cte_features_truncated = svd.transform(cte_features_all)\n",
    "# 此处注释掉的是不用tfidf， 只用frequency\n",
    "# 2000的话在cluster上大概20-30min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\xe5\\xa6\\x82\\xe6\\x9e\\x9cSVD\\xe6\\x98\\xaf2000\\xe7\\x9a\\x84\\xe8\\xaf\\x9dbigram\\xef\\xbc\\x8c\\xe5\\xb9\\xb6\\xe6\\x97\\xa0\\xe6\\x8f\\x90\\xe5\\x8d\\x87\\n Model  TF-IDF Accuracy\\n0  BernoulliNB         0.735526\\n1   Perceptron         0.820614\\n2          SVC         0.843860\\n3     Logistic         0.837281\\n\\n\\xe5\\xa6\\x82\\xe6\\x9e\\x9cSVD=100bigram\\xef\\xbc\\x8c\\xe6\\xaf\\x94200\\xe9\\x99\\x8d\\xe4\\xba\\x86\\nModel  TF-IDF Accuracy\\n0  BernoulliNB         0.735088\\n1   Perceptron         0.799561\\n2          SVC         0.810526\\n3     Logistic         0.807456\\n\\n\\xe5\\xa6\\x82\\xe6\\x9e\\x9cSVD=300\\xef\\xbc\\x8c\\xe5\\x85\\xb6\\xe4\\xbb\\x96\\xe6\\xb2\\xa1\\xe5\\x95\\xa5\\xe6\\x95\\x88\\xe6\\x9e\\x9c\\xef\\xbc\\x8cperceptron\\xe9\\x99\\x8d\\xe4\\xba\\x86\\xe4\\xb8\\x8d\\xe5\\xb0\\x91\\nModel  TF-IDF Accuracy\\n0  BernoulliNB         0.750877\\n1   Perceptron         0.746930\\n2          SVC         0.829386\\n3     Logistic         0.832895\\n('Classification report for ', 'Perceptron')\\n             precision    recall  f1-score   support\\n\\n        neg       0.69      0.97      0.81      1247\\n        pos       0.92      0.48      0.63      1033\\n\\navg / total       0.80      0.75      0.73      2280\\n\\xe6\\x9c\\x80\\xe7\\xbb\\x88ensemble\\nneg       0.91      0.79      0.85      1441\\n        pos       0.71      0.87      0.78       839\\n\\navg / total       0.84      0.82      0.82      2280\\n\\n\\n\\xe8\\xbf\\x98\\xe6\\x98\\xaf\\xe7\\x94\\xa8200\\xe5\\x90\\xa7\\n\""
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "如果SVD是2000的话bigram，并无提升\n",
    " Model  TF-IDF Accuracy\n",
    "0  BernoulliNB         0.735526\n",
    "1   Perceptron         0.820614\n",
    "2          SVC         0.843860\n",
    "3     Logistic         0.837281\n",
    "\n",
    "如果SVD=100bigram，比200降了\n",
    "Model  TF-IDF Accuracy\n",
    "0  BernoulliNB         0.735088\n",
    "1   Perceptron         0.799561\n",
    "2          SVC         0.810526\n",
    "3     Logistic         0.807456\n",
    "\n",
    "如果SVD=300，其他没啥效果，perceptron降了不少\n",
    "Model  TF-IDF Accuracy\n",
    "0  BernoulliNB         0.750877\n",
    "1   Perceptron         0.746930\n",
    "2          SVC         0.829386\n",
    "3     Logistic         0.832895\n",
    "('Classification report for ', 'Perceptron')\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "        neg       0.69      0.97      0.81      1247\n",
    "        pos       0.92      0.48      0.63      1033\n",
    "\n",
    "avg / total       0.80      0.75      0.73      2280\n",
    "最终ensemble\n",
    "neg       0.91      0.79      0.85      1441\n",
    "        pos       0.71      0.87      0.78       839\n",
    "\n",
    "avg / total       0.84      0.82      0.82      2280\n",
    "\n",
    "\n",
    "还是用200吧\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here searchgrid for optimal parameter， 或者直接用500的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# 'MultinomialNB': MultinomialNB(),\n",
    "models = {\n",
    "          'Logistic' : linear_model.LogisticRegression(C=1e5),\n",
    "          'SVC' : SVC(C=10000),\n",
    "        'Perceptron': linear_model.Perceptron(n_iter=1000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model  TF-IDF Accuracy\n",
      "0  Perceptron         0.336627\n",
      "1         SVC         0.954008\n",
      "2    Logistic         0.955948\n"
     ]
    }
   ],
   "source": [
    "# 不用SVD反而差了\n",
    "'''\n",
    "Model  TF-IDF Accuracy\n",
    "0  BernoulliNB         0.702632\n",
    "1   Perceptron         0.809649\n",
    "2          SVC         0.771491\n",
    "3     Logistic         0.832018\n",
    "'''\n",
    "results_svd = pd.DataFrame()\n",
    "\n",
    "foldnum = 0\n",
    "tfprediction = {}\n",
    "cprediction = {}\n",
    "for name,model in models.items():\n",
    "        #model.fit(tr_features_all, Train_labels)\n",
    "        #tfprediction[name] = model.predict(te_features_all)\n",
    "        model.fit(tr_features_truncated, Train_labels)\n",
    "        tfprediction[name] = model.predict(te_features_truncated)\n",
    "        tfaccuracy = metrics.accuracy_score(tfprediction[name],Test_labels)\n",
    "        \n",
    "        #model.fit(ctr_features_truncated,Train_labels)\n",
    "        #cprediction[name] = model.predict(cte_features_truncated)\n",
    "        #caccuracy = metrics.accuracy_score(cprediction[name],Test_labels)\n",
    "\n",
    "        results_svd.loc[foldnum,'Model']=name\n",
    "        results_svd.loc[foldnum,'TF-IDF Accuracy']=tfaccuracy\n",
    "        #results_svd.loc[foldnum,'Count Accuracy']=caccuracy\n",
    "        foldnum = foldnum+1\n",
    "print (results_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for name,model in models.items():\n",
    "    print (\"Classification report for \",name)\n",
    "    print(metrics.classification_report(Test_labels, tfprediction[name]))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "clf2 = linear_model.LogisticRegression(C=1e5)\n",
    "clf3 = SVC(C=10000)\n",
    "eclf = VotingClassifier(estimators=[ ('SVC', clf3),('Logistic', clf2)], voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('SVC', SVC(C=10000, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)), ('Logistic', LogisticRegression(C=100000.0, class_we...ty='l2', random_state=None,\n",
       "          solver='liblinear', tol=0.0001, verbose=0, warm_start=False))],\n",
       "         voting='hard', weights=None)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eclf.fit(tr_features_all, Train_labels)\n",
    "eclf.fit(tr_features_truncated, Train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.13      0.59      0.22       111\n",
      "        pos       1.00      0.96      0.98     10717\n",
      "\n",
      "avg / total       0.99      0.96      0.97     10828\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\xe8\\xbf\\x99\\xe4\\xb8\\xaa\\xe6\\x98\\xafsvd=200, bigram\\xe7\\x9a\\x84\\xe7\\xbb\\x93\\xe6\\x9e\\x9c\\nprecision    recall  f1-score   support\\n\\n        neg       0.86      0.83      0.85      1282\\n        pos       0.79      0.82      0.81       998\\n\\navg / total       0.83      0.83      0.83      2280\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfaccuracy = metrics.classification_report(eclf.predict(te_features_all),Test_labels)\n",
    "tfaccuracy = metrics.classification_report(eclf.predict(te_features_truncated),Test_labels)\n",
    "print tfaccuracy\n",
    "\n",
    "'''\n",
    "这个是svd=200, bigram的结果\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "        neg       0.86      0.83      0.85      1282\n",
    "        pos       0.79      0.82      0.81       998\n",
    "\n",
    "avg / total       0.83      0.83      0.83      2280\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get important features\n",
    "# count_vect.vocabulary_.get(u'algorithm')\n",
    "# 这里小小作弊一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n",
      "pos\n",
      "pos\n",
      "pos\n",
      "pos\n",
      "pos\n",
      "pos\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['nice', 'amazing', 'excellent', 'like', 'good', 'love', 'pos']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'nice amazing excellent, Like, good, love'\n",
    "def find_features(text):\n",
    "    text = preprocessing(text)\n",
    "    important_features = []\n",
    "    comments_new = list(text)\n",
    "    vec_comments_new = vec_all.transform(comments_new)\n",
    "    tfidf_comments_new = tf_vec_all.transform(vec_comments_new)\n",
    "    \n",
    "    tfidf_comments_new_truncated = svd.transform(tfidf_comments_new)\n",
    "    final_decision = eclf.predict(tfidf_comments_new_truncated)[0]\n",
    "    \n",
    "    #final_decision = eclf.predict(tfidf_comments_new)[0]\n",
    "    \n",
    "    print final_decision\n",
    "    text_list = text.split()\n",
    "    for i in text_list:\n",
    "        word = list(i)\n",
    "        vec_word = vec_all.transform(word)\n",
    "        \n",
    "        tfidf_word = tf_vec_all.transform(vec_word)\n",
    "        \n",
    "        tfidf_word_truncated = svd.transform(tfidf_word)\n",
    "        print eclf.predict(tfidf_word_truncated)[0]\n",
    "        #print eclf.predict(tfidf_word)[0]\n",
    "        \n",
    "        if eclf.predict(tfidf_word_truncated)[0]==final_decision:\n",
    "        #if eclf.predict(tfidf_word)[0]==final_decision:\n",
    "            important_features.append(i)\n",
    "    important_features.append(final_decision)\n",
    "    return important_features\n",
    "# so the last element of important_features would be the classification result \n",
    "find_features(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.pkl']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.externals import joblib\n",
    "# dump to pickle\n",
    "joblib.dump(eclf, 'model.pkl', compress=1)\n",
    "\n",
    "# and reload from pickle\n",
    "#eclf = joblib.load('model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
